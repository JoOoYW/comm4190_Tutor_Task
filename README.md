# LLMs and Tutoring, can they do it?

Sometimes chatbots can be distracted and latch onto any sort of interpretation. A long chat may become overridden with nonsense, filled with a bunch of emojis, and somewhere in between is the answer to your question. In this experiment, the chatbot is asked about the same topic (give or take). In one "session", the chatbot is not instructed at all and in the other, the chatbot is given explicit instructions to become a tutor. 
